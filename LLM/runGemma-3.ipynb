{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/huggingface/transformers@v4.49.0-Gemma-3\n",
        "!pip install huggingface_hub\n",
        "!pip install -U bitsandbytes\n",
        "# !pip install --upgrade torch torchvision torchaudio\n",
        "# !pip install --upgrade transformers"
      ],
      "metadata": {
        "id": "phHYclCZjLKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ivuhu61yiip_"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "import torch\n",
        "from huggingface_hub import login\n",
        "\n",
        "login(token=\"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\")\n",
        "\n",
        "pipe = pipeline(\"text-generation\",\n",
        "                model=\"google/gemma-3-1b-it\",\n",
        "                device=\"cuda\",\n",
        "                torch_dtype=torch.bfloat16\n",
        "                )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"TRANSFORMERS_NO_SDPA\"] = \"1\" #menonaktifkan SDPA.\n",
        "from transformers import AutoTokenizer, BitsAndBytesConfig, Gemma3ForCausalLM\n",
        "import torch\n",
        "\n",
        "model_id = \"google/gemma-3-1b-it\"\n",
        "\n",
        "# quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "\n",
        "# model = Gemma3ForCausalLM.from_pretrained(\n",
        "#     model_id, quantization_config=quantization_config\n",
        "# ).eval()\n",
        "\n",
        "# Hapus konfigurasi kuantisasi\n",
        "model = Gemma3ForCausalLM.from_pretrained(model_id).eval()\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "\n",
        "\n",
        "messages = [\n",
        "    [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": [{\"type\": \"text\", \"text\": \"You are a creative anime background tag generator.\"},]\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [{\"type\": \"text\", \"text\": \"Generate a list of place/background tags for anime stable diffusion prompt.\"},]\n",
        "        },\n",
        "    ],\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt=True,\n",
        "    tokenize=True,\n",
        "    return_dict=True,\n",
        "    return_tensors=\"pt\",\n",
        ").to(model.device)\n",
        "\n",
        "# Konversi input_ids ke torch.long\n",
        "inputs[\"input_ids\"] = inputs[\"input_ids\"].long()\n",
        "\n",
        "# Konversi semua Tensor lainnya ke torch.bfloat16\n",
        "for key in inputs:\n",
        "    if key != \"input_ids\":\n",
        "        inputs[key] = inputs[key].to(torch.bfloat16)\n",
        "\n",
        "with torch.inference_mode():\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=256,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        temperature=1.2,\n",
        "        top_p=0.9,\n",
        "        repetition_penalty=1.1,\n",
        "        do_sample=True,\n",
        "    )\n",
        "\n",
        "outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "print(outputs[0])"
      ],
      "metadata": {
        "id": "-Uqo2upw7lHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = outputs[0]\n",
        "\n",
        "start_marker = \"Hereâ€™s a breakdown, categorized by feel & style:\"\n",
        "end_marker = \"**Tips for Using These Tags Effectively:**\"\n",
        "start_index = result.find(start_marker) + len(start_marker)\n",
        "end_index = result.find(end_marker)\n",
        "\n",
        "result = result[start_index:end_index]\n",
        "\n",
        "print(result)\n",
        "\n",
        "textFile = open(f\"prompt.txt\", \"a\")\n",
        "textFile.write(\"\\n\")\n",
        "textFile.write(result)\n",
        "textFile.close()"
      ],
      "metadata": {
        "id": "vyzv2qpU-pm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import datetime\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "x = datetime.datetime.now().strftime(\"%Y_%m_%d\")\n",
        "!mkdir \"/content/drive/MyDrive/Colab Notebooks/{x}\"\n",
        "\n",
        "!mkdir \"/content/drive/MyDrive/Colab Notebooks/{x}/prompt\"\n",
        "\n",
        "!mv \"/content/\"*.txt \"/content/drive/MyDrive/Colab Notebooks/{x}/prompt\"\n"
      ],
      "metadata": {
        "id": "OI50tc8UVQT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "mN_So1Ac3u7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numba\n",
        "from numba import cuda\n",
        "device = cuda.get_current_device()\n",
        "device.reset()"
      ],
      "metadata": {
        "id": "VEFvWZEX5VO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi\n",
        "# !kill 0"
      ],
      "metadata": {
        "id": "Oje__GKe4csb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}